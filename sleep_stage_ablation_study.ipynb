{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Stage Prediction - Ablation Study\n",
    "## Comparing HR-only vs HR+Motion vs HR+Motion+Steps\n",
    "\n",
    "This notebook allows you to toggle different data modalities to see their impact on prediction accuracy.\n",
    "\n",
    "**Dataset**: [Motion and Heart Rate from Wrist-Worn Wearable](https://physionet.org/content/sleep-accel/1.0.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q optuna xgboost scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ablation Study Configuration\n",
    "\n",
    "**Toggle these flags to run different experiments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# ABLATION STUDY CONFIGURATION\n",
    "# ================================================================\n",
    "\n",
    "# Data modalities to include\n",
    "INCLUDE_HR = True        # Heart rate features (baseline)\n",
    "INCLUDE_MOTION = True    # Accelerometer features (recommended!)\n",
    "INCLUDE_STEPS = False    # Step count features (marginal benefit)\n",
    "\n",
    "# Classification settings\n",
    "NUM_CLASSES = 5          # 5 = full (Wake/N1/N2/N3/REM), 3 = simplified (Wake/NREM/REM)\n",
    "USE_CLASS_WEIGHTS = False\n",
    "\n",
    "# Optuna settings\n",
    "N_TRIALS = 100\n",
    "\n",
    "# Print config\n",
    "print(\"=\"*50)\n",
    "print(\"ABLATION STUDY CONFIG\")\n",
    "print(\"=\"*50)\n",
    "print(f\"  Heart Rate:  {'ON' if INCLUDE_HR else 'OFF'}\")\n",
    "print(f\"  Motion:      {'ON' if INCLUDE_MOTION else 'OFF'}\")\n",
    "print(f\"  Steps:       {'ON' if INCLUDE_STEPS else 'OFF'}\")\n",
    "print(f\"  Classes:     {NUM_CLASSES}\")\n",
    "print(f\"  Trials:      {N_TRIALS}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Dataset\n",
    "\n",
    "⚠️ **Warning**: Motion data is ~2GB. Only downloads what's needed based on config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./sleep_data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "DATASET_URL = \"https://physionet.org/files/sleep-accel/1.0.0\"\n",
    "\n",
    "# Always need labels\n",
    "print(\"Downloading labels...\")\n",
    "!wget -q -r -np -nH --cut-dirs=3 -P {DATA_DIR} {DATASET_URL}/labels/\n",
    "\n",
    "# Heart rate (small, ~4MB)\n",
    "if INCLUDE_HR:\n",
    "    print(\"Downloading heart rate data (~4MB)...\")\n",
    "    !wget -q -r -np -nH --cut-dirs=3 -P {DATA_DIR} {DATASET_URL}/heart_rate/\n",
    "\n",
    "# Motion (large, ~2GB)\n",
    "if INCLUDE_MOTION:\n",
    "    print(\"Downloading motion data (~2GB - this will take a while)...\")\n",
    "    !wget -q -r -np -nH --cut-dirs=3 -P {DATA_DIR} {DATASET_URL}/motion/\n",
    "\n",
    "# Steps (small)\n",
    "if INCLUDE_STEPS:\n",
    "    print(\"Downloading steps data...\")\n",
    "    !wget -q -r -np -nH --cut-dirs=3 -P {DATA_DIR} {DATASET_URL}/steps/\n",
    "\n",
    "print(\"\\nDownload complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEART_RATE_DIR = DATA_DIR / \"heart_rate\"\n",
    "MOTION_DIR = DATA_DIR / \"motion\"\n",
    "STEPS_DIR = DATA_DIR / \"steps\"\n",
    "LABELS_DIR = DATA_DIR / \"labels\"\n",
    "\n",
    "SLEEP_STAGE_NAMES = {\n",
    "    0: 'Wake',\n",
    "    1: 'N1',\n",
    "    2: 'N2',\n",
    "    3: 'N3',\n",
    "    5: 'REM'\n",
    "}\n",
    "\n",
    "EPOCH_DURATION = 30  # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_ids() -> List[str]:\n",
    "    \"\"\"Get list of all subject IDs from labels directory.\"\"\"\n",
    "    label_files = glob.glob(str(LABELS_DIR / \"*_labeled_sleep.txt\"))\n",
    "    return sorted([Path(f).stem.replace('_labeled_sleep', '') for f in label_files])\n",
    "\n",
    "\n",
    "def load_sleep_labels(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load labeled sleep data for a subject.\"\"\"\n",
    "    label_file = LABELS_DIR / f\"{subject_id}_labeled_sleep.txt\"\n",
    "    df = pd.read_csv(label_file, sep=' ', header=None, names=['time_offset', 'sleep_stage'])\n",
    "    df['subject_id'] = subject_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_heart_rate_data(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load heart rate data for a subject.\"\"\"\n",
    "    hr_file = HEART_RATE_DIR / f\"{subject_id}_heartrate.txt\"\n",
    "    if not hr_file.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(hr_file, header=None, names=['timestamp', 'heart_rate'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_motion_data(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load accelerometer data for a subject.\"\"\"\n",
    "    motion_file = MOTION_DIR / f\"{subject_id}_acceleration.txt\"\n",
    "    if not motion_file.exists():\n",
    "        return None\n",
    "    # Motion data is space-separated: timestamp, x, y, z\n",
    "    df = pd.read_csv(motion_file, sep=' ', header=None, \n",
    "                     names=['timestamp', 'accel_x', 'accel_y', 'accel_z'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_steps_data(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load step count data for a subject.\"\"\"\n",
    "    steps_file = STEPS_DIR / f\"{subject_id}_steps.txt\"\n",
    "    if not steps_file.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(steps_file, header=None, names=['timestamp', 'steps'])\n",
    "    return df\n",
    "\n",
    "\n",
    "subject_ids = get_subject_ids()\n",
    "print(f\"Found {len(subject_ids)} subjects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hr_features(hr_values: np.ndarray, timestamps: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Extract HRV features from heart rate data within an epoch.\"\"\"\n",
    "    features = {}\n",
    "    prefix = 'hr_'\n",
    "    \n",
    "    if len(hr_values) < 2:\n",
    "        return {\n",
    "            f'{prefix}mean': np.nan, f'{prefix}std': np.nan, f'{prefix}min': np.nan,\n",
    "            f'{prefix}max': np.nan, f'{prefix}range': np.nan, f'{prefix}median': np.nan,\n",
    "            f'{prefix}rmssd': np.nan, f'{prefix}pnn50': np.nan, f'{prefix}slope': np.nan,\n",
    "            f'{prefix}count': len(hr_values), f'{prefix}cv': np.nan,\n",
    "            f'{prefix}skew': np.nan, f'{prefix}iqr': np.nan\n",
    "        }\n",
    "    \n",
    "    features[f'{prefix}mean'] = np.mean(hr_values)\n",
    "    features[f'{prefix}std'] = np.std(hr_values)\n",
    "    features[f'{prefix}min'] = np.min(hr_values)\n",
    "    features[f'{prefix}max'] = np.max(hr_values)\n",
    "    features[f'{prefix}range'] = features[f'{prefix}max'] - features[f'{prefix}min']\n",
    "    features[f'{prefix}median'] = np.median(hr_values)\n",
    "    features[f'{prefix}count'] = len(hr_values)\n",
    "    \n",
    "    features[f'{prefix}cv'] = features[f'{prefix}std'] / features[f'{prefix}mean'] if features[f'{prefix}mean'] > 0 else np.nan\n",
    "    \n",
    "    q75, q25 = np.percentile(hr_values, [75, 25])\n",
    "    features[f'{prefix}iqr'] = q75 - q25\n",
    "    \n",
    "    if features[f'{prefix}std'] > 0:\n",
    "        features[f'{prefix}skew'] = np.mean(((hr_values - features[f'{prefix}mean']) / features[f'{prefix}std']) ** 3)\n",
    "    else:\n",
    "        features[f'{prefix}skew'] = np.nan\n",
    "    \n",
    "    hr_diff = np.diff(hr_values)\n",
    "    features[f'{prefix}rmssd'] = np.sqrt(np.mean(hr_diff ** 2)) if len(hr_diff) > 0 else np.nan\n",
    "    features[f'{prefix}pnn50'] = np.sum(np.abs(hr_diff) > 5) / len(hr_diff) if len(hr_diff) > 0 else np.nan\n",
    "    \n",
    "    if len(timestamps) >= 2:\n",
    "        try:\n",
    "            slope, _ = np.polyfit(timestamps - timestamps[0], hr_values, 1)\n",
    "            features[f'{prefix}slope'] = slope\n",
    "        except:\n",
    "            features[f'{prefix}slope'] = np.nan\n",
    "    else:\n",
    "        features[f'{prefix}slope'] = np.nan\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_motion_features(accel_x: np.ndarray, accel_y: np.ndarray, \n",
    "                            accel_z: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract motion features from accelerometer data within an epoch.\n",
    "    \n",
    "    These features are critical for distinguishing Wake from REM:\n",
    "    - Wake: movement present\n",
    "    - REM: muscle atonia (paralysis), no movement\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    prefix = 'motion_'\n",
    "    \n",
    "    if len(accel_x) < 2:\n",
    "        return {\n",
    "            f'{prefix}mag_mean': np.nan, f'{prefix}mag_std': np.nan,\n",
    "            f'{prefix}mag_max': np.nan, f'{prefix}mag_min': np.nan,\n",
    "            f'{prefix}activity_count': np.nan, f'{prefix}pct_still': np.nan,\n",
    "            f'{prefix}zero_crossings': np.nan, f'{prefix}entropy': np.nan,\n",
    "            f'{prefix}x_std': np.nan, f'{prefix}y_std': np.nan, f'{prefix}z_std': np.nan,\n",
    "            f'{prefix}count': len(accel_x)\n",
    "        }\n",
    "    \n",
    "    # Compute magnitude: sqrt(x² + y² + z²)\n",
    "    magnitude = np.sqrt(accel_x**2 + accel_y**2 + accel_z**2)\n",
    "    \n",
    "    # Basic magnitude stats\n",
    "    features[f'{prefix}mag_mean'] = np.mean(magnitude)\n",
    "    features[f'{prefix}mag_std'] = np.std(magnitude)\n",
    "    features[f'{prefix}mag_max'] = np.max(magnitude)\n",
    "    features[f'{prefix}mag_min'] = np.min(magnitude)\n",
    "    features[f'{prefix}count'] = len(magnitude)\n",
    "    \n",
    "    # Activity count (classic actigraphy metric)\n",
    "    # Sum of absolute differences in magnitude\n",
    "    mag_diff = np.abs(np.diff(magnitude))\n",
    "    features[f'{prefix}activity_count'] = np.sum(mag_diff)\n",
    "    \n",
    "    # Percentage of time \"still\" (magnitude change < threshold)\n",
    "    still_threshold = 0.01  # Adjust based on sensor units\n",
    "    features[f'{prefix}pct_still'] = np.mean(mag_diff < still_threshold)\n",
    "    \n",
    "    # Zero crossings (movement frequency indicator)\n",
    "    # Count sign changes in the de-meaned signal\n",
    "    mag_centered = magnitude - np.mean(magnitude)\n",
    "    zero_crossings = np.sum(np.abs(np.diff(np.sign(mag_centered))) > 0)\n",
    "    features[f'{prefix}zero_crossings'] = zero_crossings / len(magnitude)\n",
    "    \n",
    "    # Entropy (randomness of movement)\n",
    "    # Higher entropy = more random movement (awake)\n",
    "    # Lower entropy = regular/no movement (sleep)\n",
    "    hist, _ = np.histogram(magnitude, bins=10, density=True)\n",
    "    hist = hist[hist > 0]  # Remove zeros for log\n",
    "    features[f'{prefix}entropy'] = -np.sum(hist * np.log(hist + 1e-10))\n",
    "    \n",
    "    # Per-axis variability\n",
    "    features[f'{prefix}x_std'] = np.std(accel_x)\n",
    "    features[f'{prefix}y_std'] = np.std(accel_y)\n",
    "    features[f'{prefix}z_std'] = np.std(accel_z)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_steps_features(steps_df: pd.DataFrame, epoch_start: float, \n",
    "                           epoch_end: float) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract step features for an epoch.\n",
    "    Steps are in 10-minute windows, so we interpolate/estimate.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    prefix = 'steps_'\n",
    "    \n",
    "    if steps_df is None or len(steps_df) == 0:\n",
    "        return {f'{prefix}count': np.nan, f'{prefix}rate': np.nan}\n",
    "    \n",
    "    # Find steps records that overlap with this epoch\n",
    "    # Steps are in 600-second (10 min) windows\n",
    "    window_size = 600\n",
    "    \n",
    "    overlapping = steps_df[\n",
    "        (steps_df['timestamp'] <= epoch_end) & \n",
    "        (steps_df['timestamp'] + window_size >= epoch_start)\n",
    "    ]\n",
    "    \n",
    "    if len(overlapping) == 0:\n",
    "        features[f'{prefix}count'] = 0\n",
    "        features[f'{prefix}rate'] = 0\n",
    "    else:\n",
    "        # Estimate steps in this 30-second epoch\n",
    "        # Proportional allocation from 10-minute window\n",
    "        total_steps = overlapping['steps'].sum()\n",
    "        # Scale from 10-min windows to 30-sec epoch\n",
    "        features[f'{prefix}count'] = total_steps * (EPOCH_DURATION / window_size)\n",
    "        features[f'{prefix}rate'] = features[f'{prefix}count'] / EPOCH_DURATION * 60  # steps/min\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Epoch Alignment and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process all data for a single subject, aligning to 30-second epochs.\n",
    "    \"\"\"\n",
    "    # Load labels (required)\n",
    "    labels_df = load_sleep_labels(subject_id)\n",
    "    \n",
    "    # Load optional data based on config\n",
    "    hr_df = load_heart_rate_data(subject_id) if INCLUDE_HR else None\n",
    "    motion_df = load_motion_data(subject_id) if INCLUDE_MOTION else None\n",
    "    steps_df = load_steps_data(subject_id) if INCLUDE_STEPS else None\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for _, row in labels_df.iterrows():\n",
    "        epoch_start = row['time_offset']\n",
    "        epoch_end = epoch_start + EPOCH_DURATION\n",
    "        \n",
    "        epoch_features = {\n",
    "            'time_offset': epoch_start,\n",
    "            'sleep_stage': row['sleep_stage'],\n",
    "            'subject_id': subject_id\n",
    "        }\n",
    "        \n",
    "        # Extract HR features\n",
    "        if INCLUDE_HR and hr_df is not None:\n",
    "            epoch_hr = hr_df[\n",
    "                (hr_df['timestamp'] >= epoch_start) &\n",
    "                (hr_df['timestamp'] < epoch_end)\n",
    "            ]\n",
    "            if len(epoch_hr) > 0:\n",
    "                hr_features = extract_hr_features(\n",
    "                    epoch_hr['heart_rate'].values,\n",
    "                    epoch_hr['timestamp'].values\n",
    "                )\n",
    "            else:\n",
    "                hr_features = extract_hr_features(np.array([]), np.array([]))\n",
    "            epoch_features.update(hr_features)\n",
    "        \n",
    "        # Extract motion features\n",
    "        if INCLUDE_MOTION and motion_df is not None:\n",
    "            epoch_motion = motion_df[\n",
    "                (motion_df['timestamp'] >= epoch_start) &\n",
    "                (motion_df['timestamp'] < epoch_end)\n",
    "            ]\n",
    "            if len(epoch_motion) > 0:\n",
    "                motion_features = extract_motion_features(\n",
    "                    epoch_motion['accel_x'].values,\n",
    "                    epoch_motion['accel_y'].values,\n",
    "                    epoch_motion['accel_z'].values\n",
    "                )\n",
    "            else:\n",
    "                motion_features = extract_motion_features(\n",
    "                    np.array([]), np.array([]), np.array([])\n",
    "                )\n",
    "            epoch_features.update(motion_features)\n",
    "        \n",
    "        # Extract steps features\n",
    "        if INCLUDE_STEPS:\n",
    "            steps_features = extract_steps_features(steps_df, epoch_start, epoch_end)\n",
    "            epoch_features.update(steps_features)\n",
    "        \n",
    "        features_list.append(epoch_features)\n",
    "    \n",
    "    return pd.DataFrame(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prepare Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare dataset from all subjects.\"\"\"\n",
    "    subject_ids = get_subject_ids()\n",
    "    print(f\"Processing {len(subject_ids)} subjects...\")\n",
    "    \n",
    "    all_features = []\n",
    "    \n",
    "    for i, subject_id in enumerate(subject_ids):\n",
    "        print(f\"  [{i+1}/{len(subject_ids)}] {subject_id}\", end=\"\")\n",
    "        \n",
    "        try:\n",
    "            features_df = process_subject(subject_id)\n",
    "            all_features.append(features_df)\n",
    "            print(f\" - {len(features_df)} epochs\")\n",
    "        except Exception as e:\n",
    "            print(f\" - ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    dataset = pd.concat(all_features, ignore_index=True)\n",
    "    \n",
    "    # Filter invalid sleep stages\n",
    "    valid_stages = [0, 1, 2, 3, 5]\n",
    "    dataset = dataset[dataset['sleep_stage'].isin(valid_stages)].copy()\n",
    "    \n",
    "    # Get feature columns based on what we included\n",
    "    feature_prefixes = []\n",
    "    if INCLUDE_HR:\n",
    "        feature_prefixes.append('hr_')\n",
    "    if INCLUDE_MOTION:\n",
    "        feature_prefixes.append('motion_')\n",
    "    if INCLUDE_STEPS:\n",
    "        feature_prefixes.append('steps_')\n",
    "    \n",
    "    # Drop rows with too many missing values\n",
    "    if INCLUDE_HR:\n",
    "        dataset = dataset[dataset['hr_count'] >= 2].copy()\n",
    "        dataset = dataset.dropna(subset=['hr_mean', 'hr_std'])\n",
    "    if INCLUDE_MOTION:\n",
    "        dataset = dataset[dataset['motion_count'] >= 10].copy()\n",
    "        dataset = dataset.dropna(subset=['motion_mag_mean'])\n",
    "    \n",
    "    # Add temporal context features\n",
    "    print(\"\\nAdding temporal context features...\")\n",
    "    dataset = dataset.sort_values(['subject_id', 'time_offset']).reset_index(drop=True)\n",
    "    \n",
    "    # HR temporal features\n",
    "    if INCLUDE_HR:\n",
    "        dataset['hr_mean_roll_5'] = dataset.groupby('subject_id')['hr_mean'].transform(\n",
    "            lambda x: x.rolling(window=5, center=True, min_periods=1).mean()\n",
    "        )\n",
    "        for lag in [1, 2, 4]:\n",
    "            dataset[f'hr_mean_lag_{lag}'] = dataset.groupby('subject_id')['hr_mean'].shift(lag)\n",
    "        for lead in [1, 2]:\n",
    "            dataset[f'hr_mean_lead_{lead}'] = dataset.groupby('subject_id')['hr_mean'].shift(-lead)\n",
    "        dataset['hr_diff_1'] = dataset['hr_mean'] - dataset.groupby('subject_id')['hr_mean'].shift(1)\n",
    "    \n",
    "    # Motion temporal features\n",
    "    if INCLUDE_MOTION:\n",
    "        dataset['motion_mag_roll_5'] = dataset.groupby('subject_id')['motion_mag_mean'].transform(\n",
    "            lambda x: x.rolling(window=5, center=True, min_periods=1).mean()\n",
    "        )\n",
    "        dataset['motion_activity_roll_5'] = dataset.groupby('subject_id')['motion_activity_count'].transform(\n",
    "            lambda x: x.rolling(window=5, center=True, min_periods=1).mean()\n",
    "        )\n",
    "        for lag in [1, 2]:\n",
    "            dataset[f'motion_activity_lag_{lag}'] = dataset.groupby('subject_id')['motion_activity_count'].shift(lag)\n",
    "        dataset['motion_activity_diff_1'] = dataset['motion_activity_count'] - dataset.groupby('subject_id')['motion_activity_count'].shift(1)\n",
    "    \n",
    "    # Drop NaNs from temporal features\n",
    "    dataset = dataset.dropna()\n",
    "    \n",
    "    print(f\"\\nTotal epochs: {len(dataset)}\")\n",
    "    print(f\"\\nClass distribution:\")\n",
    "    for stage, name in SLEEP_STAGE_NAMES.items():\n",
    "        count = (dataset['sleep_stage'] == stage).sum()\n",
    "        pct = count / len(dataset) * 100\n",
    "        print(f\"  {name}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "dataset = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show features\n",
    "feature_cols = [c for c in dataset.columns if any(c.startswith(p) for p in ['hr_', 'motion_', 'steps_'])]\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "for col in sorted(feature_cols):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optuna Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_objective(X, y, groups, n_folds=5, use_class_weights=False):\n",
    "    \"\"\"Create Optuna objective function.\"\"\"\n",
    "    \n",
    "    sample_weights = None\n",
    "    if use_class_weights:\n",
    "        class_weights = {}\n",
    "        for cls in np.unique(y):\n",
    "            class_weights[cls] = len(y) / (len(np.unique(y)) * np.sum(y == cls))\n",
    "        sample_weights = np.array([class_weights[yi] for yi in y])\n",
    "    \n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(np.unique(y)),\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'booster': 'gbtree',\n",
    "            'device': 'cuda',\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        cv = GroupKFold(n_splits=n_folds)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y, groups)):\n",
    "            X_train, X_val = X[train_idx], X[val_idx]\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            weights_train = sample_weights[train_idx] if sample_weights is not None else None\n",
    "            \n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train, y_train, sample_weight=weights_train,\n",
    "                     eval_set=[(X_val, y_val)], verbose=False)\n",
    "            \n",
    "            y_pred = model.predict(X_val)\n",
    "            f1 = f1_score(y_val, y_pred, average='macro')\n",
    "            f1_scores.append(f1)\n",
    "            \n",
    "            trial.report(np.mean(f1_scores), fold_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        return np.mean(f1_scores)\n",
    "    \n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = dataset[feature_cols].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "if NUM_CLASSES == 3:\n",
    "    print(\"*** Using 3-class mode: Wake / NREM / REM ***\")\n",
    "    y_raw = dataset['sleep_stage'].replace({1: 2, 3: 2}).values\n",
    "    SLEEP_STAGE_NAMES_USED = {0: 'Wake', 2: 'NREM', 5: 'REM'}\n",
    "else:\n",
    "    y_raw = dataset['sleep_stage'].values\n",
    "    SLEEP_STAGE_NAMES_USED = SLEEP_STAGE_NAMES\n",
    "\n",
    "y = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "subject_encoder = LabelEncoder()\n",
    "groups = subject_encoder.fit_transform(dataset['subject_id'].values)\n",
    "\n",
    "# Fill NaN\n",
    "for i in range(X.shape[1]):\n",
    "    col_median = np.nanmedian(X[:, i])\n",
    "    X[np.isnan(X[:, i]), i] = col_median if not np.isnan(col_median) else 0\n",
    "\n",
    "print(f\"Features: {X.shape}\")\n",
    "print(f\"Classes: {[SLEEP_STAGE_NAMES_USED.get(c, c) for c in label_encoder.classes_]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=42),\n",
    "    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=2)\n",
    ")\n",
    "\n",
    "objective = create_objective(X, y, groups, n_folds=5, use_class_weights=USE_CLASS_WEIGHTS)\n",
    "\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Best trial:\")\n",
    "print(f\"  Macro F1: {study.best_trial.value:.4f}\")\n",
    "print(f\"  Params: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "cv = GroupKFold(n_splits=5)\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "\n",
    "for train_idx, val_idx in cv.split(X, y, groups):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    params = study.best_params.copy()\n",
    "    params['objective'] = 'multi:softmax'\n",
    "    params['num_class'] = len(np.unique(y))\n",
    "    params['random_state'] = 42\n",
    "    params['verbosity'] = 0\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    all_y_true.extend(y_val)\n",
    "    all_y_pred.extend(y_pred)\n",
    "\n",
    "stage_names = [SLEEP_STAGE_NAMES_USED.get(s, str(s)) for s in label_encoder.classes_]\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ABLATION STUDY RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Data: HR={'ON' if INCLUDE_HR else 'OFF'}, Motion={'ON' if INCLUDE_MOTION else 'OFF'}, Steps={'ON' if INCLUDE_STEPS else 'OFF'}\")\n",
    "print(f\"Classes: {NUM_CLASSES}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nOverall Accuracy: {accuracy_score(all_y_true, all_y_pred):.4f}\")\n",
    "print(f\"Macro F1 Score:   {f1_score(all_y_true, all_y_pred, average='macro'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_y_true, all_y_pred, target_names=stage_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=stage_names, yticklabels=stage_names, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=stage_names, yticklabels=stage_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.suptitle(f\"HR={'ON' if INCLUDE_HR else 'OFF'}, Motion={'ON' if INCLUDE_MOTION else 'OFF'}, Steps={'ON' if INCLUDE_STEPS else 'OFF'}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "final_params = study.best_params.copy()\n",
    "final_params['objective'] = 'multi:softmax'\n",
    "final_params['num_class'] = len(np.unique(y))\n",
    "final_params['random_state'] = 42\n",
    "final_params['verbosity'] = 0\n",
    "\n",
    "final_model = xgb.XGBClassifier(**final_params)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Plot importance\n",
    "importance = final_model.feature_importances_\n",
    "sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_n = min(25, len(importance))\n",
    "plt.barh(range(top_n), importance[sorted_idx[:top_n]][::-1])\n",
    "plt.yticks(range(top_n), [feature_cols[i] for i in sorted_idx[:top_n]][::-1])\n",
    "plt.xlabel('Importance')\n",
    "plt.title(f'Top {top_n} Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 15 features:\")\n",
    "for i in sorted_idx[:15]:\n",
    "    print(f\"  {feature_cols[i]}: {importance[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results filename based on config\n",
    "config_str = f\"hr{int(INCLUDE_HR)}_motion{int(INCLUDE_MOTION)}_steps{int(INCLUDE_STEPS)}_{NUM_CLASSES}class\"\n",
    "\n",
    "# Save\n",
    "pd.DataFrame([study.best_params]).to_csv(f'best_params_{config_str}.csv', index=False)\n",
    "final_model.save_model(f'model_{config_str}.json')\n",
    "study.trials_dataframe().to_csv(f'trials_{config_str}.csv', index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'config': config_str,\n",
    "    'include_hr': INCLUDE_HR,\n",
    "    'include_motion': INCLUDE_MOTION,\n",
    "    'include_steps': INCLUDE_STEPS,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'accuracy': accuracy_score(all_y_true, all_y_pred),\n",
    "    'macro_f1': f1_score(all_y_true, all_y_pred, average='macro'),\n",
    "    'n_features': len(feature_cols),\n",
    "    'n_samples': len(y)\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(f'summary_{config_str}.csv', index=False)\n",
    "\n",
    "print(f\"Results saved with prefix: {config_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(f'summary_{config_str}.csv')\n",
    "    files.download(f'model_{config_str}.json')\n",
    "except:\n",
    "    print(\"Files saved to current directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
