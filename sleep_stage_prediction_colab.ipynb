{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Stage Prediction using Heart Rate Data\n",
    "## Optuna + XGBoost Hyperparameter Optimization\n",
    "\n",
    "This notebook predicts sleep stages (Wake, N1, N2, N3, REM) from heart rate data using:\n",
    "- **XGBoost** for classification\n",
    "- **Optuna** for hyperparameter tuning\n",
    "- **HRV features** extracted from heart rate time series\n",
    "\n",
    "**Dataset**: [Motion and Heart Rate from Wrist-Worn Wearable](https://physionet.org/content/sleep-accel/1.0.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q optuna xgboost scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset from PhysioNet\n",
    "\n",
    "The dataset is hosted on PhysioNet. We'll download only the heart rate and labels folders (not motion data which is 2GB+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory\n",
    "DATA_DIR = Path(\"./sleep_data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Download dataset using wget\n",
    "# Note: PhysioNet requires accepting data use agreement for some datasets\n",
    "DATASET_URL = \"https://physionet.org/files/sleep-accel/1.0.0\"\n",
    "\n",
    "# Download heart_rate folder\n",
    "!wget -q -r -np -nH --cut-dirs=3 -P {DATA_DIR} {DATASET_URL}/heart_rate/\n",
    "\n",
    "# Download labels folder  \n",
    "!wget -q -r -np -nH --cut-dirs=3 -P {DATA_DIR} {DATASET_URL}/labels/\n",
    "\n",
    "print(\"Download complete!\")\n",
    "print(f\"\\nHeart rate files: {len(list((DATA_DIR / 'heart_rate').glob('*.txt')))}\")\n",
    "print(f\"Label files: {len(list((DATA_DIR / 'labels').glob('*.txt')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Upload from Google Drive\n",
    "\n",
    "If you've already downloaded the dataset, you can mount Google Drive and point to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if using Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# DATA_DIR = Path('/content/drive/MyDrive/path/to/your/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "HEART_RATE_DIR = DATA_DIR / \"heart_rate\"\n",
    "LABELS_DIR = DATA_DIR / \"labels\"\n",
    "\n",
    "# Sleep stage mapping\n",
    "SLEEP_STAGE_NAMES = {\n",
    "    0: 'Wake',\n",
    "    1: 'N1',\n",
    "    2: 'N2',\n",
    "    3: 'N3',\n",
    "    5: 'REM'\n",
    "}\n",
    "\n",
    "# PSG epoch duration\n",
    "EPOCH_DURATION = 30  # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_heart_rate_data(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load heart rate data for a subject.\"\"\"\n",
    "    hr_file = HEART_RATE_DIR / f\"{subject_id}_heartrate.txt\"\n",
    "    df = pd.read_csv(hr_file, header=None, names=['timestamp', 'heart_rate'])\n",
    "    df['subject_id'] = subject_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_sleep_labels(subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Load labeled sleep data for a subject.\"\"\"\n",
    "    label_file = LABELS_DIR / f\"{subject_id}_labeled_sleep.txt\"\n",
    "    df = pd.read_csv(label_file, sep=' ', header=None, names=['time_offset', 'sleep_stage'])\n",
    "    df['subject_id'] = subject_id\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_subject_ids() -> List[str]:\n",
    "    \"\"\"Get list of all subject IDs.\"\"\"\n",
    "    hr_files = glob.glob(str(HEART_RATE_DIR / \"*_heartrate.txt\"))\n",
    "    return sorted([Path(f).stem.replace('_heartrate', '') for f in hr_files])\n",
    "\n",
    "\n",
    "# Test loading\n",
    "subject_ids = get_subject_ids()\n",
    "print(f\"Found {len(subject_ids)} subjects\")\n",
    "print(f\"Subject IDs: {subject_ids[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HRV Feature Extraction\n",
    "\n",
    "We extract heart rate variability (HRV) features from each 30-second epoch:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| hr_mean, hr_std, hr_median | Basic statistics |\n",
    "| hr_min, hr_max, hr_range | Range metrics |\n",
    "| hr_cv, hr_iqr, hr_skew | Distribution metrics |\n",
    "| hr_rmssd | Root Mean Square of Successive Differences |\n",
    "| hr_pnn50 | Proportion of successive differences > 5 BPM |\n",
    "| hr_slope | Linear trend within epoch |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hrv_features(hr_values: np.ndarray, timestamps: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract HRV features from heart rate data within an epoch.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    if len(hr_values) < 2:\n",
    "        return {\n",
    "            'hr_mean': np.nan, 'hr_std': np.nan, 'hr_min': np.nan,\n",
    "            'hr_max': np.nan, 'hr_range': np.nan, 'hr_median': np.nan,\n",
    "            'hr_rmssd': np.nan, 'hr_pnn50': np.nan, 'hr_slope': np.nan,\n",
    "            'hr_count': len(hr_values), 'hr_cv': np.nan,\n",
    "            'hr_skew': np.nan, 'hr_iqr': np.nan\n",
    "        }\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['hr_mean'] = np.mean(hr_values)\n",
    "    features['hr_std'] = np.std(hr_values)\n",
    "    features['hr_min'] = np.min(hr_values)\n",
    "    features['hr_max'] = np.max(hr_values)\n",
    "    features['hr_range'] = features['hr_max'] - features['hr_min']\n",
    "    features['hr_median'] = np.median(hr_values)\n",
    "    features['hr_count'] = len(hr_values)\n",
    "    \n",
    "    # Coefficient of variation\n",
    "    features['hr_cv'] = features['hr_std'] / features['hr_mean'] if features['hr_mean'] > 0 else np.nan\n",
    "    \n",
    "    # IQR\n",
    "    q75, q25 = np.percentile(hr_values, [75, 25])\n",
    "    features['hr_iqr'] = q75 - q25\n",
    "    \n",
    "    # Skewness\n",
    "    if features['hr_std'] > 0:\n",
    "        features['hr_skew'] = np.mean(((hr_values - features['hr_mean']) / features['hr_std']) ** 3)\n",
    "    else:\n",
    "        features['hr_skew'] = np.nan\n",
    "    \n",
    "    # HRV-like features\n",
    "    hr_diff = np.diff(hr_values)\n",
    "    \n",
    "    # RMSSD\n",
    "    features['hr_rmssd'] = np.sqrt(np.mean(hr_diff ** 2)) if len(hr_diff) > 0 else np.nan\n",
    "    \n",
    "    # pNN50 (adapted: % of successive differences > 5 BPM)\n",
    "    features['hr_pnn50'] = np.sum(np.abs(hr_diff) > 5) / len(hr_diff) if len(hr_diff) > 0 else np.nan\n",
    "    \n",
    "    # Slope\n",
    "    if len(timestamps) >= 2:\n",
    "        try:\n",
    "            slope, _ = np.polyfit(timestamps - timestamps[0], hr_values, 1)\n",
    "            features['hr_slope'] = slope\n",
    "        except:\n",
    "            features['hr_slope'] = np.nan\n",
    "    else:\n",
    "        features['hr_slope'] = np.nan\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Align HR Data to Sleep Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_hr_to_epochs(hr_df: pd.DataFrame, labels_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Align heart rate data to 30-second sleep epochs.\n",
    "    \"\"\"\n",
    "    label_start = labels_df['time_offset'].min()\n",
    "    label_end = labels_df['time_offset'].max() + EPOCH_DURATION\n",
    "    \n",
    "    hr_df_filtered = hr_df[\n",
    "        (hr_df['timestamp'] >= label_start - EPOCH_DURATION) &\n",
    "        (hr_df['timestamp'] <= label_end + EPOCH_DURATION)\n",
    "    ].copy()\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    for _, row in labels_df.iterrows():\n",
    "        epoch_start = row['time_offset']\n",
    "        epoch_end = epoch_start + EPOCH_DURATION\n",
    "        \n",
    "        epoch_hr = hr_df_filtered[\n",
    "            (hr_df_filtered['timestamp'] >= epoch_start) &\n",
    "            (hr_df_filtered['timestamp'] < epoch_end)\n",
    "        ]\n",
    "        \n",
    "        if len(epoch_hr) > 0:\n",
    "            features = extract_hrv_features(\n",
    "                epoch_hr['heart_rate'].values,\n",
    "                epoch_hr['timestamp'].values\n",
    "            )\n",
    "        else:\n",
    "            features = extract_hrv_features(np.array([]), np.array([]))\n",
    "        \n",
    "        features['time_offset'] = epoch_start\n",
    "        features['sleep_stage'] = row['sleep_stage']\n",
    "        features['subject_id'] = row['subject_id']\n",
    "        features_list.append(features)\n",
    "    \n",
    "    return pd.DataFrame(features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_dataset() -> pd.DataFrame:\n    \"\"\"Load and prepare dataset from all subjects.\"\"\"\n    subject_ids = get_subject_ids()\n    print(f\"Processing {len(subject_ids)} subjects...\")\n    \n    all_features = []\n    \n    for i, subject_id in enumerate(subject_ids):\n        print(f\"  [{i+1}/{len(subject_ids)}] {subject_id}\", end=\"\")\n        \n        try:\n            hr_df = load_heart_rate_data(subject_id)\n            labels_df = load_sleep_labels(subject_id)\n            features_df = align_hr_to_epochs(hr_df, labels_df)\n            all_features.append(features_df)\n            print(f\" - {len(features_df)} epochs\")\n        except Exception as e:\n            print(f\" - ERROR: {e}\")\n            continue\n    \n    dataset = pd.concat(all_features, ignore_index=True)\n    \n    # Filter invalid sleep stages\n    valid_stages = [0, 1, 2, 3, 5]\n    dataset = dataset[dataset['sleep_stage'].isin(valid_stages)].copy()\n    \n    # Drop rows with insufficient HR samples\n    dataset = dataset[dataset['hr_count'] >= 2].copy()\n    dataset = dataset.dropna(subset=['hr_mean', 'hr_std'])\n    \n    # =============================================================\n    # ADD TEMPORAL CONTEXT FEATURES (Critical for sleep staging!)\n    # =============================================================\n    print(\"\\nAdding temporal context features...\")\n    \n    # Sort by subject and time (CRITICAL for lag/lead to work correctly)\n    dataset = dataset.sort_values(['subject_id', 'time_offset']).reset_index(drop=True)\n    \n    # 1. Rolling averages (trend context) - 5 epochs = 2.5 minutes window\n    dataset['hr_mean_roll_5'] = dataset.groupby('subject_id')['hr_mean'].transform(\n        lambda x: x.rolling(window=5, center=True, min_periods=1).mean()\n    )\n    dataset['hr_std_roll_5'] = dataset.groupby('subject_id')['hr_std'].transform(\n        lambda x: x.rolling(window=5, center=True, min_periods=1).mean()\n    )\n    \n    # 2. Lag features (past context) - what happened before?\n    for lag in [1, 2, 4]:  # 30s, 60s, 2min ago\n        dataset[f'hr_mean_lag_{lag}'] = dataset.groupby('subject_id')['hr_mean'].shift(lag)\n        dataset[f'hr_std_lag_{lag}'] = dataset.groupby('subject_id')['hr_std'].shift(lag)\n    \n    # 3. Lead features (future context) - what happens next? (valid for offline analysis)\n    for lead in [1, 2]:  # 30s, 60s ahead\n        dataset[f'hr_mean_lead_{lead}'] = dataset.groupby('subject_id')['hr_mean'].shift(-lead)\n    \n    # 4. Rate of change (is HR dropping or rising?)\n    dataset['hr_diff_1'] = dataset['hr_mean'] - dataset.groupby('subject_id')['hr_mean'].shift(1)\n    dataset['hr_diff_2'] = dataset['hr_mean'] - dataset.groupby('subject_id')['hr_mean'].shift(2)\n    \n    # 5. Variability change\n    dataset['hr_std_diff_1'] = dataset['hr_std'] - dataset.groupby('subject_id')['hr_std'].shift(1)\n    \n    # Drop NaNs created by shifting (edges of each subject's recording)\n    dataset = dataset.dropna()\n    \n    print(f\"\\nTotal epochs: {len(dataset)}\")\n    print(f\"\\nClass distribution:\")\n    for stage, name in SLEEP_STAGE_NAMES.items():\n        count = (dataset['sleep_stage'] == stage).sum()\n        pct = count / len(dataset) * 100\n        print(f\"  {name}: {count} ({pct:.1f}%)\")\n    \n    return dataset\n\n\n# Load dataset\ndataset = prepare_dataset()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ================================================================\n# CONFIGURATION - Adjust these settings\n# ================================================================\nUSE_CLASS_WEIGHTS = False  # Set True to balance classes (hurts accuracy without motion data)\nNUM_CLASSES = 5            # Set to 3 for simplified Wake/NREM/REM classification\n\ndef create_objective(X, y, groups, n_folds=5, use_class_weights=False):\n    \"\"\"\n    Create Optuna objective function for XGBoost tuning.\n    Uses GroupKFold to prevent subject leakage.\n    \"\"\"\n    \n    # Optionally calculate class weights\n    sample_weights = None\n    if use_class_weights:\n        class_weights = {}\n        for cls in np.unique(y):\n            class_weights[cls] = len(y) / (len(np.unique(y)) * np.sum(y == cls))\n        sample_weights = np.array([class_weights[yi] for yi in y])\n    \n    def objective(trial):\n        # Hyperparameter search space\n        params = {\n            'objective': 'multi:softmax',\n            'num_class': len(np.unique(y)),\n            'eval_metric': 'mlogloss',\n            'booster': 'gbtree',  # Fixed to gbtree (dart is 10-50x slower)\n            'device': 'cuda',     # Use GPU (Colab T4/A100)\n            'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n            'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n            'max_depth': trial.suggest_int('max_depth', 3, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n            'random_state': 42,\n            'verbosity': 0\n        }\n        \n        # Cross-validation with GroupKFold\n        cv = GroupKFold(n_splits=n_folds)\n        f1_scores = []\n        \n        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X, y, groups)):\n            X_train, X_val = X[train_idx], X[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n            weights_train = sample_weights[train_idx] if sample_weights is not None else None\n            \n            model = xgb.XGBClassifier(**params)\n            model.fit(X_train, y_train, sample_weight=weights_train,\n                     eval_set=[(X_val, y_val)],\n                     verbose=False)\n            \n            y_pred = model.predict(X_val)\n            f1 = f1_score(y_val, y_pred, average='macro')\n            f1_scores.append(f1)\n            \n            # Pruning\n            trial.report(np.mean(f1_scores), fold_idx)\n            if trial.should_prune():\n                raise optuna.TrialPruned()\n        \n        return np.mean(f1_scores)\n    \n    return objective"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Optuna Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare features and labels\nfeature_cols = [c for c in dataset.columns if c.startswith('hr_')]\nX = dataset[feature_cols].values\n\nprint(f\"Features ({len(feature_cols)} total): {feature_cols}\")\n\n# Encode labels\nlabel_encoder = LabelEncoder()\n\n# Optional: Simplify to 3 classes (Wake/NREM/REM) for testing\nif NUM_CLASSES == 3:\n    print(\"\\n*** Using simplified 3-class mode: Wake / NREM / REM ***\")\n    # Map N1, N2, N3 -> NREM (use value 2)\n    y_raw = dataset['sleep_stage'].replace({1: 2, 3: 2}).values\n    SLEEP_STAGE_NAMES_USED = {0: 'Wake', 2: 'NREM', 5: 'REM'}\nelse:\n    y_raw = dataset['sleep_stage'].values\n    SLEEP_STAGE_NAMES_USED = SLEEP_STAGE_NAMES\n\ny = label_encoder.fit_transform(y_raw)\n\n# Subject groups\nsubject_encoder = LabelEncoder()\ngroups = subject_encoder.fit_transform(dataset['subject_id'].values)\n\n# Fill NaN with column medians\nfor i in range(X.shape[1]):\n    col_median = np.nanmedian(X[:, i])\n    X[np.isnan(X[:, i]), i] = col_median\n\nprint(f\"\\nFeatures shape: {X.shape}\")\nprint(f\"Classes: {label_encoder.classes_} -> {[SLEEP_STAGE_NAMES_USED.get(c, c) for c in label_encoder.classes_]}\")\nprint(f\"Subjects: {len(np.unique(groups))}\")\nprint(f\"Class weights: {'ENABLED' if USE_CLASS_WEIGHTS else 'DISABLED'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create and run study\nN_TRIALS = 100  # Adjust based on compute budget\n\nstudy = optuna.create_study(\n    direction='maximize',\n    sampler=TPESampler(seed=42),\n    pruner=MedianPruner(n_startup_trials=10, n_warmup_steps=2)\n)\n\nobjective = create_objective(X, y, groups, n_folds=5, use_class_weights=USE_CLASS_WEIGHTS)\n\nstudy.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Best trial:\")\nprint(f\"  Macro F1: {study.best_trial.value:.4f}\")\nprint(f\"  Params: {study.best_trial.params}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final evaluation with GroupKFold\ncv = GroupKFold(n_splits=5)\nall_y_true = []\nall_y_pred = []\n\nfor train_idx, val_idx in cv.split(X, y, groups):\n    X_train, X_val = X[train_idx], X[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n    \n    params = study.best_params.copy()\n    params['objective'] = 'multi:softmax'\n    params['num_class'] = len(np.unique(y))\n    params['random_state'] = 42\n    params['verbosity'] = 0\n    \n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)  # No sample weights for final eval\n    \n    y_pred = model.predict(X_val)\n    all_y_true.extend(y_val)\n    all_y_pred.extend(y_pred)\n\n# Classification report\nstage_names = [SLEEP_STAGE_NAMES_USED.get(s, str(s)) for s in label_encoder.classes_]\nprint(\"Classification Report:\")\nprint(classification_report(all_y_true, all_y_pred, target_names=stage_names))\n\n# Also show accuracy\nfrom sklearn.metrics import accuracy_score\nprint(f\"\\nOverall Accuracy: {accuracy_score(all_y_true, all_y_pred):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=stage_names, yticklabels=stage_names, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=stage_names, yticklabels=stage_names, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model on all data\n",
    "final_params = study.best_params.copy()\n",
    "final_params['objective'] = 'multi:softmax'\n",
    "final_params['num_class'] = len(np.unique(y))\n",
    "final_params['random_state'] = 42\n",
    "final_params['verbosity'] = 0\n",
    "\n",
    "final_model = xgb.XGBClassifier(**final_params)\n",
    "final_model.fit(X, y, sample_weight=sample_weights)\n",
    "\n",
    "# Plot feature importance\n",
    "importance = final_model.feature_importances_\n",
    "sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(importance)), importance[sorted_idx])\n",
    "plt.xticks(range(len(importance)), [feature_cols[i] for i in sorted_idx], rotation=45, ha='right')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop features:\")\n",
    "for i in sorted_idx[:5]:\n",
    "    print(f\"  {feature_cols[i]}: {importance[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Optuna Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization history\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter importance\n",
    "fig = plot_param_importances(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best parameters\n",
    "pd.DataFrame([study.best_params]).to_csv('best_params.csv', index=False)\n",
    "\n",
    "# Save model\n",
    "final_model.save_model('sleep_stage_model.json')\n",
    "\n",
    "# Save optimization history\n",
    "study.trials_dataframe().to_csv('optimization_history.csv', index=False)\n",
    "\n",
    "print(\"Results saved!\")\n",
    "print(\"  - best_params.csv\")\n",
    "print(\"  - sleep_stage_model.json\")\n",
    "print(\"  - optimization_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('best_params.csv')\n",
    "    files.download('sleep_stage_model.json')\n",
    "    files.download('optimization_history.csv')\n",
    "except:\n",
    "    print(\"Not running in Colab - files saved to current directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}